{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ca13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle\n",
    "import urllib\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import Linear \n",
    "\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "# import random\n",
    "from scipy.stats import hypergeom\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be95c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_from_numpy(x, device):\n",
    "    return torch.from_numpy(x).to(device)\n",
    "\n",
    "def plot_loss_with_acc(loss_history, val_acc_history):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.plot(range(len(loss_history)), loss_history,\n",
    "             c=np.array([255, 71, 90]) / 255.)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Training Loss ')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb24943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class HalfAutoEncoder(nn.Module):\n",
    "    def __init__(self,linear4,linear5,linear6 ):\n",
    "        super(HalfAutoEncoder,self).__init__()\n",
    "        \n",
    "        self.linear4= linear4\n",
    "        self.linear5= linear5\n",
    "        self.linear6= linear6\n",
    "\n",
    "    \n",
    "    def forward(self, h3):\n",
    "        self.h4  = F.relu(self.linear4(h3))\n",
    "        self.h4 = F.normalize(self.h4)\n",
    "        \n",
    "        self.h5  = F.relu(self.linear5(self.h4))\n",
    "        self.h5 = F.normalize(self.h5)\n",
    "        \n",
    "        self.h6  = self.linear6(self.h5)\n",
    "\n",
    "        return  self.h6\n",
    "\n",
    "# class HalfAutoEncoder(nn.Module):\n",
    "#     def __init__(self,linear5,linear6,linear7,linear8 ):\n",
    "#         super(HalfAutoEncoder,self).__init__()\n",
    "         \n",
    "#         self.linear5= linear5\n",
    "#         self.linear6= linear6\n",
    "#         self.linear7= linear7\n",
    "#         self.linear8= linear8\n",
    "    \n",
    "#     def forward(self, h4):\n",
    "#         self.h5  = F.relu(self.linear5( h4))\n",
    "#         self.h5 = F.normalize(self.h5)\n",
    "        \n",
    "#         self.h6  = F.sigmoid(self.linear6( self.h5))\n",
    "#         self.h6 = F.normalize(self.h6)\n",
    "        \n",
    "#         self.h7  = F.relu(self.linear7(self.h6))\n",
    "#         self.h7 = F.normalize(self.h7)\n",
    "        \n",
    "#         self.h8  = self.linear8(self.h7)\n",
    "\n",
    "#         return  self.h8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac26805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数定义\n",
    "# LEARNING_RATE = 1e-3 #0.001\n",
    "# WEIGHT_DACAY = 1e-4\n",
    "# EPOCHS = 80\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249b4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_np = np.load(\"../model/feacture_np.npy\")\n",
    " \n",
    "tensor_x = tensor_from_numpy(feature_np , DEVICE)\n",
    "\n",
    "model = torch.load( '../model/half_auto_encoder.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "labelList=pd.read_csv('../data/label.csv',header=None)[1]\n",
    "for i in range(0,len(labelList)):\n",
    "    labelList[i]+=1\n",
    "   \n",
    "tensor_y = tensor_from_numpy(np.load(\"../model/rec_X.npy\"), DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e21504e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroOneEncoder():\n",
    "    def __init__(self,model,tensor_x, tensor_y):\n",
    "        self.model=model\n",
    "        self.tensor_x=tensor_x\n",
    "        self.tensor_mask=tensor_x\n",
    "        self.tensor_y=tensor_y\n",
    "        self.zero_one_feature=np.zeros(shape=tensor_x.shape)\n",
    "        self.tensor_train_mask=[False  for i in range(0,850)]\n",
    "        self.criterion=nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    def run(self):\n",
    "#         for i in range(0,tensor_x.shape[0]):\n",
    "        for i in range(0,200):\n",
    "            \n",
    "            self.tensor_train_mask[i]=True # 一次只取一条数据出来计算loss\n",
    "#             logits = self.model(self.tensor_x)  # 前向传播\n",
    "#             train_mask_logits = logits[self.tensor_train_mask]   # 只选择训练节点进行监督\n",
    "#             loss = self.criterion(train_mask_logits, tensor_y[self.tensor_train_mask])   #计算每一条数据的loss\n",
    "            \n",
    "            \n",
    "            # 对每一条数据的特征一个个进行mask,看看哪个特征被mask以后引起loss剧烈变化,就置为1, 否则置为0\n",
    "            loss_hist=[]\n",
    "            for j in range(0,self.tensor_x.shape[1]):\n",
    "                 \n",
    "                self.tensor_mask[i][j]=0 #对第i条数据的第j 个特征进行 mask\n",
    "                logits = self.model(self.tensor_mask)  # 前向传播\n",
    "                train_mask_logits = logits[self.tensor_train_mask]   # 只选择训练节点进行监督\n",
    "                mask_loss = self.criterion(train_mask_logits, tensor_y[self.tensor_train_mask])   #计算每一条数据的loss\n",
    "#                 loss_hist.append( abs(mask_loss.item()-loss.item())/loss.item() )\n",
    "                loss_hist.append( mask_loss.item() )\n",
    "                self.tensor_mask[i][j]=self.tensor_x[i][j] #把被mask的地方还原回来\n",
    "             \n",
    "            thresh = np.percentile(loss_hist, 80)\n",
    "\n",
    "#             print(loss_hist)\n",
    "            for j in range(0,self.tensor_x.shape[1]):\n",
    "                if loss_hist[j] > thresh:\n",
    "                    self.zero_one_feature[i][j]=1\n",
    "                    \n",
    "            self.tensor_train_mask[i]=False\n",
    "\n",
    "            \n",
    "        return self.zero_one_feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df9b3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_one_encoder=ZeroOneEncoder(model,tensor_x, tensor_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee706ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zero_one_feature=zero_one_encoder.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaefd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"../model/zero_one_feature.npy\",zero_one_feature )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.array(labelList)==4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a6ce9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 1.\n",
      "  1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      "  1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      "  1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 0\n",
    "print( zero_one_feature[46: 47] )\n",
    "print( zero_one_feature[64: 65] )\n",
    "print( zero_one_feature[75: 76] )\n",
    "print( zero_one_feature[81: 82] )\n",
    "print( zero_one_feature[86: 87] )\n",
    "print( zero_one_feature[93: 94] )\n",
    "print( zero_one_feature[103: 104] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8671efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "print( zero_one_feature[0: 1] )\n",
    "print( zero_one_feature[26: 27] )\n",
    "print( zero_one_feature[30: 31] )\n",
    "print( zero_one_feature[32: 33] )\n",
    "print( zero_one_feature[39: 40] )\n",
    "print( zero_one_feature[40: 41] )\n",
    "print( zero_one_feature[49: 50] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bc60efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    " # 2\n",
    "print( zero_one_feature[1: 2] )\n",
    "print( zero_one_feature[5: 6] )\n",
    "print( zero_one_feature[7: 8] )\n",
    "print( zero_one_feature[8: 9] )\n",
    "# print( zero_one_feature[12: 13] )\n",
    "# print( zero_one_feature[15: 16] )\n",
    "# print( zero_one_feature[16: 17] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3125339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      "  1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      "  1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      "  1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 3 33,  37,  52, 100, 112, 133, 150\n",
    "print( zero_one_feature[33: 34] )\n",
    "print( zero_one_feature[37: 38] )\n",
    "print( zero_one_feature[52: 53] )\n",
    "print( zero_one_feature[100: 101] )\n",
    "print( zero_one_feature[112: 113] )\n",
    "print( zero_one_feature[133: 134] )\n",
    "print( zero_one_feature[150: 151] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b94d53f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      "  1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n",
      "[[0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 4   2,  3,  27,  31,  45,  51, 62\n",
    "print( zero_one_feature[2: 3] )\n",
    "print( zero_one_feature[3: 4] )\n",
    "print( zero_one_feature[27: 28] )\n",
    "print( zero_one_feature[31: 32] )\n",
    "print( zero_one_feature[45: 46] )\n",
    "print( zero_one_feature[51: 52] )\n",
    "print( zero_one_feature[62: 63] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f5bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac18da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1ba7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_one_feature = np.load(\"../model/zero_one_feature.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_np=zero_one_feature[0:200]\n",
    "# feature_np/=feature_np.sum(1, keepdims=True)\n",
    "kmeans = KMeans(n_clusters=12,init='k-means++').fit(feature_np)\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "print('purity : ',purity_score(labelList[0:200],kmeans.labels_ ))\n",
    "# print('ARI : ',adjusted_rand_score(labelList,kmeans.labels_))\n",
    "# print('NMI : ',normalized_mutual_info_score(labelList,kmeans.labels_ ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1139a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 利用 超几何分布的 生存函数计算 相似度\n",
    "\n",
    "num_pairs = int(zero_one_feature.shape[0]*(zero_one_feature.shape[0]-1)/2)\n",
    "\n",
    "\n",
    "ab_vec = np.zeros((zero_one_feature.shape[0],  ))  #记录特征为1 的个数\n",
    "activate_dict = {}# 储存为 特征矩阵 为1 的索引\n",
    " \n",
    "for i in range(zero_one_feature.shape[0]):\n",
    "    ab_vec[i] = np.sum(zero_one_feature[i])\n",
    "    activate_dict[i] = np.where(zero_one_feature[i] == 1)\n",
    " \n",
    "\n",
    "e = np.log10(num_pairs)\n",
    "\n",
    "\n",
    "abc_dict = {}\n",
    "def sf(a, b, c):\n",
    "    key = (a, b, c)\n",
    "    if not key in abc_dict:\n",
    "#         val = -np.log10(stats.hypergeom.sf(c - 1, zero_one_feature.shape[1], a, b)) - e\n",
    " \n",
    "        val = stats.hypergeom.pmf(c , zero_one_feature.shape[1], a, b)\n",
    " \n",
    "        abc_dict[key] = val\n",
    "        abc_dict[(b, a, c)] = val\n",
    "    else:\n",
    "        val = abc_dict[key]\n",
    "    return val\n",
    "\n",
    "np_adj=np.zeros(shape=[zero_one_feature.shape[0],zero_one_feature.shape[0]])\n",
    "\n",
    "with tqdm(total=num_pairs) as pbar:\n",
    "    for i in range(zero_one_feature.shape[0]):\n",
    "        for j in range(i+1, zero_one_feature.shape[0]):\n",
    "            pbar.update(1)\n",
    "            a = ab_vec[i]#为1 的个数\n",
    "            b = ab_vec[j]#为1 的个数\n",
    "            c = np.intersect1d(activate_dict[i], activate_dict[j]).shape[0] # 都为1 的索引相同的个数\n",
    "\n",
    "            similar= stats.hypergeom.pmf(c , zero_one_feature.shape[1], a, b)\n",
    "#             similar= sf(a,b,c)\n",
    "#             if similar>0:\n",
    "            np_adj[i][j]=similar\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,zero_one_feature.shape[0]):\n",
    "    for j in range(0,i):\n",
    "\n",
    "        np_adj[i][j]=np_adj[j][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "np_adj/=np_adj.sum(1, keepdims=True)  \n",
    "        \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markov_clustering as mc\n",
    "\n",
    "matrix = sp.csr_matrix(np.matrix(np_adj))\n",
    "\n",
    "result = mc.run_mcl(matrix)\n",
    "clusters = mc.get_clusters(result) \n",
    "Q = mc.modularity(matrix=result, clusters=clusters)\n",
    "# print(\"modularity:\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e49d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c933ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import hypergeom\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "[M, n, N] = [20, 7, 12]\n",
    "rv = hypergeom(M, n, N)\n",
    "x = np.arange(0, n+1)\n",
    "pmf_dogs = rv.pmf(x)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, pmf_dogs, 'bo')\n",
    "ax.vlines(x, 0, pmf_dogs, lw=2)\n",
    "ax.set_xlabel('# of dogs in our group of chosen animals')\n",
    "ax.set_ylabel('hypergeom PMF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dfea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "class Contig_data(data.Dataset):\n",
    "    def __init__(self, file, type='train'):\n",
    "        self.type = type\n",
    "        self.file = file\n",
    "        # The last column is label, the other columns are the features\n",
    "        df = pd.read_csv(self.file, header = None)\n",
    "        self.labels = df.values[:, -1]\n",
    "        # self.seqs = df.values[:, :-1].astype(np.float32)\n",
    "        self.seqs = df.values[:, : -1].astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.seqs.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.seqs[index], self.labels[index]\n",
    "    \n",
    "test_data = Contig_data(file='../data/xubo_kmer/kmer.csv')\n",
    "test_loader = DataLoader(test_data )\n",
    "for batch_idx, (data, _) in enumerate(test_loader):\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = np.percentile([1,1,1,1,2], 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392dd33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
